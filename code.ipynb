{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829c2ffb-67fd-4881-b030-00a6e247b119",
   "metadata": {},
   "source": [
    "# Source Code and outputs for Problem 4.3_Hw4_CSE 250A_Fall 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0844c219-f11f-4c34-a81f-cb89a654c101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum log Likelihood estimate for the unigram distribution is 8702875.844317213\n",
      "\n",
      "\n",
      "Below we print all the words starting with letter M and their numerical unigram probabilities:\n",
      "The word is MILLION with P_u(MILLION)= 0.002072759168154815 \n",
      "\n",
      "The word is MORE with P_u(MORE)= 0.0017088989966186725 \n",
      "\n",
      "The word is MR. with P_u(MR.)= 0.0014416083492816956 \n",
      "\n",
      "The word is MOST with P_u(MOST)= 0.0007879173033190295 \n",
      "\n",
      "The word is MARKET with P_u(MARKET)= 0.0007803712804681068 \n",
      "\n",
      "The word is MAY with P_u(MAY)= 0.0007298973156289532 \n",
      "\n",
      "The word is M. with P_u(M.)= 0.0007034067394618568 \n",
      "\n",
      "The word is MANY with P_u(MANY)= 0.0006967290595970209 \n",
      "\n",
      "The word is MADE with P_u(MADE)= 0.0005598610827336895 \n",
      "\n",
      "The word is MUCH with P_u(MUCH)= 0.0005145971758110562 \n",
      "\n",
      "The word is MAKE with P_u(MAKE)= 0.0005144626437991272 \n",
      "\n",
      "The word is MONTH with P_u(MONTH)= 0.00044490959363187093 \n",
      "\n",
      "The word is MONEY with P_u(MONEY)= 0.00043710673693999306 \n",
      "\n",
      "The word is MONTHS with P_u(MONTHS)= 0.0004057607781605526 \n",
      "\n",
      "The word is MY with P_u(MY)= 0.0004003183467688823 \n",
      "\n",
      "The word is MONDAY with P_u(MONDAY)= 0.00038198530259784006 \n",
      "\n",
      "The word is MAJOR with P_u(MAJOR)= 0.00037089252670515475 \n",
      "\n",
      "The word is MILITARY with P_u(MILITARY)= 0.00035204581485220204 \n",
      "\n",
      "The word is MEMBERS with P_u(MEMBERS)= 0.00033606096579846475 \n",
      "\n",
      "The word is MIGHT with P_u(MIGHT)= 0.00027358919153183117 \n",
      "\n",
      "The word is MEETING with P_u(MEETING)= 0.0002657374141083427 \n",
      "\n",
      "The word is MUST with P_u(MUST)= 0.0002665079156312084 \n",
      "\n",
      "The word is ME with P_u(ME)= 0.00026357267173457725 \n",
      "\n",
      "The word is MARCH with P_u(MARCH)= 0.0002597935452176646 \n",
      "\n",
      "The word is MAN with P_u(MAN)= 0.0002528834918776787 \n",
      "\n",
      "The word is MS. with P_u(MS.)= 0.0002389900041002911 \n",
      "\n",
      "The word is MINISTER with P_u(MINISTER)= 0.00023977273580605944 \n",
      "\n",
      "The word is MAKING with P_u(MAKING)= 0.00021170446604452378 \n",
      "\n",
      "The word is MOVE with P_u(MOVE)= 0.0002099555498894477 \n",
      "\n",
      "The word is MILES with P_u(MILES)= 0.00020596851026319035 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part a. solution\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# loading the vocabulary list as a dictionary with the line numbers in the text file 'Vocab' as keys\n",
    "v=1\n",
    "vocablist=[]\n",
    "with open('hw4_vocab.txt') as f:\n",
    "    for line in f:\n",
    "        vocablist.append(v)\n",
    "        vocablist.append(line.strip())\n",
    "        v=v+1\n",
    "\n",
    "def Convertlisttodic(lst):     # function to convert a list to dictionary\n",
    "    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return res_dct\n",
    " \n",
    "vocabdictwlinekey=Convertlisttodic(vocablist) # creates a dictionary from vocab file with keys as line numbers and words as values \n",
    "\n",
    "\n",
    "# loading the unigram counts as a dictionary with the line numbers in the text file as keys\n",
    "\n",
    "v=1\n",
    "vocabunigramlist=[]\n",
    "with open('hw4_unigram.txt') as f:\n",
    "    for line in f:\n",
    "        vocabunigramlist.append(v)\n",
    "        vocabunigramlist.append(int(line.strip()))  #takes the counts as integer values \n",
    "        v=v+1\n",
    "\n",
    "unigmdictlinekey=Convertlisttodic(vocabunigramlist) # creates a dictionary from the text file with keys as line numbers and unigram counts as values \n",
    "\n",
    "\n",
    "# computing unigram probabilities: \n",
    "s=sum(unigmdictlinekey.values())    # total count of all the words \n",
    "probdict={key: value / s for key, value in unigmdictlinekey.items()}   \n",
    "# the last line above creates probabilities/ML estimates from unigram count\n",
    "\n",
    "\n",
    "# computing maximum log likelihood = Sum over Count(X=x)*log(Unigram probability of X=x) for unigram\n",
    "maxlikelihoodunigram=0\n",
    "for key, value in unigmdictlinekey.items():\n",
    "    maxlikelihoodunigram=maxlikelihoodunigram+ value*probdict[key]\n",
    "    \n",
    "print(f\"Maximum log Likelihood estimate for the unigram distribution is {maxlikelihoodunigram}\\n\\n\")\n",
    "\n",
    "print(f\"Below we print all the words starting with letter M and their numerical unigram probabilities:\")\n",
    "    \n",
    "# printing the table of words that start with 'M' along with their numerical probabilities\n",
    "for key, value in vocabdictwlinekey.items():\n",
    "    if value[0]=='M':\n",
    "        print(f\"The word is {value} with P_u({value})= {probdict[key]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198b3b46-eb0a-4629-9c1f-30d69defb108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram total wordcount is not defined for </s>, probability will be 0 for this\n",
      "\n",
      "The 10 most frequent words being used after 'THE', along with their probabilities are:\n",
      "the word <UNK> with bigram probability 0.6150198100055118\n",
      "the word U. with bigram probability 0.013372499432610317\n",
      "the word FIRST with bigram probability 0.011720260675031612\n",
      "the word COMPANY with bigram probability 0.011658788055636611\n",
      "the word NEW with bigram probability 0.009451480076516552\n",
      "the word UNITED with bigram probability 0.008672308141231398\n",
      "the word GOVERNMENT with bigram probability 0.006803488635995202\n",
      "the word NINETEEN with bigram probability 0.006650714911000876\n",
      "the word SAME with bigram probability 0.006287066757449016\n",
      "the word TWO with bigram probability 0.006160749602827221\n"
     ]
    }
   ],
   "source": [
    "# part b. solution: Calculating bigram probabilities P_{ML}(w'|w)= count (w comes before w')/count(w)\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# loading the vocabulary list as a dictionary with the line numbers in the text file 'Vocab' as keys\n",
    "v=1\n",
    "vocablist=[]\n",
    "with open('hw4_vocab.txt') as f:\n",
    "    for line in f:\n",
    "        vocablist.append(v)\n",
    "        vocablist.append(line.strip())\n",
    "        v=v+1\n",
    "\n",
    "def Convertlisttodic(lst):     # function to convert a list to dictionary\n",
    "    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return res_dct\n",
    " \n",
    "vocabdictwlinekey=Convertlisttodic(vocablist) # creates a dictionary from vocab file with keys as line numbers and words as values \n",
    "\n",
    "# loading the bigram counts as an array : 1st column values in the array are indices for words that appear first \n",
    "# and 2nd column values in the array are indices for words that appear next. 3rd column values are numbers of such occurrences\n",
    "\n",
    "X = pd.read_csv('hw4_bigram.txt', sep=\"\\t\", header=None) # reading the table of the file using Pandas\n",
    "\n",
    "# so our array of bigramcount is :\n",
    "bigrampairscount=np.array(X.values) #: Note in the array 'bigrampairscount', indexing of rows and columns start from 0\n",
    "\n",
    "def bitotwordcount(word):     # calculates the total count of a preceeding word (w) in the vocabulary \n",
    "    countword=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==word:\n",
    "            k=key   # this key is unique for each word\n",
    "            break\n",
    "    for l in range(len(bigrampairscount[:,0])):\n",
    "        if  bigrampairscount[l,0]== k:\n",
    "            countword=countword+bigrampairscount[l,2]\n",
    "    return countword\n",
    "\n",
    "# computing total bigram ocuurences for each word that preceedes:  and storing it in a dictionary\n",
    "bitotfirstword={key: bitotwordcount(value) for key, value in vocabdictwlinekey.items()}   \n",
    "\n",
    "#checking sanity ! \n",
    "for i in range(1,501):\n",
    "    if bitotfirstword[i]==0:\n",
    "        print(f\"bigram total wordcount is not defined for {vocabdictwlinekey[i]}, probability will be 0 for this\\n\")\n",
    "    \n",
    "# making a final array of maximal likelihood, bigram probability distribution\n",
    "# it will store bigram probability P_b(w'|w) where index of w can be found from the first\n",
    "# row of 'bigrampairscount' and index of w' can be found from second row of that array\n",
    "# after getting index we get the actual words from the dictionary 'vocabdictwlinekey'.\n",
    "\n",
    "probdist=np.zeros(len(bigrampairscount[:,0]), dtype=float)\n",
    "\n",
    "for i in range(len(bigrampairscount[:,0])):\n",
    "    dummy=bitotfirstword[bigrampairscount[i,0]]\n",
    "    probdist[i]=bigrampairscount[i,2]/dummy\n",
    "\n",
    "# this probdist will give us all the probabilities of pairs (w,w') i.e. maximal likelihood P(w'|w)\n",
    "\n",
    "\n",
    "\n",
    "# if we want to make the probabilities of the pair of words as in the bigram text file then:\n",
    "\n",
    "bigramprobml=np.column_stack((bigrampairscount[:,0:2],probdist))\n",
    "# -in this last array : 1st column = index of w, second column=index of w'\n",
    "# 3rd column is P(w'|w)\n",
    "\n",
    "\n",
    "## printing 10 most likely words after 'THE' and their numerical bigram probabilities:\n",
    "\n",
    "#obtaining the key correspoding to 'THE' in our Vocab\n",
    "for key, value in vocabdictwlinekey.items():\n",
    "        if value=='THE':\n",
    "            keyofthe=key   # this key is unique for each word\n",
    "            break\n",
    "\n",
    "imin=np.min(np.where(bigrampairscount[:,0] == keyofthe)); #gets smallest index \n",
    "imax=np.max(np.where(bigrampairscount[:,0] == keyofthe)); # gets largest index\n",
    "#- where first letter is 'THE'\n",
    "\n",
    "a=bigramprobml[imin:imax+1,:]  # only taking the part where first word is 'THE'\n",
    "\n",
    "# now we do the array sorting that we need :\n",
    "b=a[a[:, 2].argsort()]   # sorts from smallest to highest as we go below\n",
    "\n",
    "print(f\"The 10 most frequent words being used after 'THE', along with their probabilities are:\")\n",
    "\n",
    "for i in range(10):\n",
    "    j=-i-1\n",
    "    wordkey=int(b[j,1])\n",
    "    print(f\"the word {vocabdictwlinekey[wordkey]} with bigram probability {b[j,2]}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455bbd88-f923-4c33-aa5a-20ca33f5806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of the sentence 'THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK' by unigram model is -64.50944034364878 \n",
      "\n",
      "The log likelihood of the sentence 'THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK' by bigram model is -40.91813213378977 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part c solution: Comparing log-likelihoods of the sentence 'The Stock Market fell by one hundred points last week'\n",
    "# under unigram and bigram models\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# loading the vocabulary list as a dictionary with the line numbers in the text file 'Vocab' as keys\n",
    "v=1\n",
    "vocablist=[]\n",
    "with open('hw4_vocab.txt') as f:\n",
    "    for line in f:\n",
    "        vocablist.append(v)\n",
    "        vocablist.append(line.strip())\n",
    "        v=v+1\n",
    "\n",
    "def Convertlisttodic(lst):     # function to convert a list to dictionary\n",
    "    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return res_dct\n",
    " \n",
    "vocabdictwlinekey=Convertlisttodic(vocablist) # creates a dictionary from vocab file with keys as line numbers and words as values \n",
    "\n",
    "##unigram\n",
    "# loading the unigram counts as a dictionary with the line numbers in the text file as keys\n",
    "\n",
    "v=1\n",
    "vocabunigramlist=[]\n",
    "with open('hw4_unigram.txt') as f:\n",
    "    for line in f:\n",
    "        vocabunigramlist.append(v)\n",
    "        vocabunigramlist.append(int(line.strip()))  #takes the counts as integer values \n",
    "        v=v+1\n",
    "\n",
    "unigmdictlinekey=Convertlisttodic(vocabunigramlist) # creates a dictionary from the text file with keys as line numbers and unigram counts as values \n",
    "\n",
    "\n",
    "# computing unigram probabilities: \n",
    "s1=0    # total count of all the words\n",
    "for key in unigmdictlinekey.keys():\n",
    "    s1=s1+unigmdictlinekey[key]\n",
    "probdictuni={key: value/s1 for key, value in unigmdictlinekey.items()}   \n",
    "# the last line above creates probabilities/ML estimates from unigram count\n",
    "\n",
    "\n",
    "def unigramprobcalc(word):\n",
    "    s=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value == word:\n",
    "            s=1\n",
    "            break\n",
    "    if s==0:\n",
    "        print('the word is not in the unigram count list/vocabulary so unigram probabilty will be 0')\n",
    "        return 0\n",
    "    return probdictuni[key]\n",
    "  \n",
    "#################################################################\n",
    "## bigram\n",
    "# loading the bigram counts as an array : 1st column values in the array are indices for words that appear first \n",
    "# and 2nd column values in the array are indices for words that appear next. 3rd column values are numbers of such occurrences\n",
    "\n",
    "X = pd.read_csv('hw4_bigram.txt', sep=\"\\t\", header=None) # reading the table of the file using Pandas\n",
    "\n",
    "# so our array of bigramcount is :\n",
    "bigrampairscount=np.array(X.values) #: Note in the array 'bigrampairscount', indexing of rows and columns start from 0\n",
    "\n",
    "def bitotwordcount(word):     # calculates the total count of a preceeding word (w) in the vocabulary \n",
    "    countword=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==word:\n",
    "            k=key   # this key is unique for each word\n",
    "            break\n",
    "    for l in range(len(bigrampairscount[:,0])):\n",
    "        if  bigrampairscount[l,0]== k:\n",
    "            countword=countword+bigrampairscount[l,2]\n",
    "    return countword\n",
    "\n",
    "# computing total bigram ocuurences for each word that preceedes:  and storing it in a dictionary\n",
    "bitotfirstword={key: bitotwordcount(value) for key, value in vocabdictwlinekey.items()}   \n",
    "\n",
    "    \n",
    "# making a final array of maximal likelihood, bigram probability distribution\n",
    "# it will store bigram probability P_b(w'|w) where index of w can be found from the first\n",
    "# row of 'bigrampairscount' and index of w' can be found from second row of that array\n",
    "# after getting index we get the actual words from the dictionary 'vocabdictwlinekey'.\n",
    "\n",
    "probdist=np.zeros(len(bigrampairscount[:,0]), dtype=float)\n",
    "\n",
    "for i in range(len(bigrampairscount[:,0])):\n",
    "    dummy=bitotfirstword[bigrampairscount[i,0]]\n",
    "    probdist[i]=bigrampairscount[i,2]/dummy\n",
    "\n",
    "# this probdist will give us all the probabilities of pairs (w,w') i.e. maximal likelihood P(w'|w)\n",
    "\n",
    "# if we want to make the probabilities of the pair of words as in the bigram text file then:\n",
    "\n",
    "bigramprob=np.column_stack((bigrampairscount[:,0:2],probdist))\n",
    "# -in this last array : 1st column = index of w, second column=index of w'\n",
    "# 3rd column is P(w'|w)\n",
    "\n",
    "def findkey(i,j):      # finds the probability of a part w, w' with indices i, j respectively: \n",
    "    t=0\n",
    "    for m in range(len(bigramprob[:,0])):\n",
    "        if bigramprob[m,0]==i and bigramprob[m,1]==j:\n",
    "            t=1\n",
    "            break\n",
    "    if t==1:\n",
    "        return m\n",
    "    else:\n",
    "        return len(bigramprob[:,0])+5\n",
    "    \n",
    "\n",
    "def bigramprobcalc(wordfirst, wordsecond): # calculates P(wordsecond|wordfirst) from bigram data\n",
    "    s=0\n",
    "    keyfirst=0\n",
    "    keysecond=0    #our keys for words start from 1 so 0 is a safe value\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==wordfirst:\n",
    "            keyfirst=key\n",
    "            break\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==wordsecond:\n",
    "            keysecond=key\n",
    "            break\n",
    "    if keyfirst!=0 and keysecond!=0:\n",
    "        #assuming uniqueness of a pair in the bigram text file\n",
    "        f=findkey(keyfirst,keysecond)\n",
    "        if f !=len(bigramprob[:,0])+5:\n",
    "            s=bigramprob[f,2]\n",
    "        \n",
    "    return s\n",
    "\n",
    "\n",
    "####################################### Actual calculation of part c of the problem##############\n",
    "\n",
    "# the sentence : \"THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK. \"\n",
    "\n",
    "p1=[unigramprobcalc('THE'),unigramprobcalc('STOCK'),unigramprobcalc('MARKET'),unigramprobcalc('FELL'),unigramprobcalc('BY'),unigramprobcalc('ONE'),unigramprobcalc('HUNDRED'),unigramprobcalc('POINTS'),unigramprobcalc('LAST'),unigramprobcalc('WEEK')]\n",
    "\n",
    "p2=[bigramprobcalc('<s>', 'THE'),bigramprobcalc('THE', 'STOCK'),bigramprobcalc('STOCK', 'MARKET'),bigramprobcalc('MARKET', 'FELL'),bigramprobcalc('FELL', 'BY' ),bigramprobcalc('BY', 'ONE'),bigramprobcalc('ONE', 'HUNDRED'),bigramprobcalc('HUNDRED', 'POINTS'),bigramprobcalc('POINTS', 'LAST'),bigramprobcalc('LAST', 'WEEK')]\n",
    "\n",
    "Uni=np.prod(p1)\n",
    "\n",
    "Bi=np.prod(p2)\n",
    "\n",
    "if Uni==0:\n",
    "    print('Unigram total probability 0: atleast one word in the sentence is not in our vocabulary or unigram count list\\n')\n",
    "    \n",
    "if Bi==0:\n",
    "    print('Bigram total probability 0: atleast one {word, next word} pair in the sentence is not in our vocabulary or bigram count list\\n')\n",
    "\n",
    "print(f\"The log likelihood of the sentence 'THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK' by unigram model is {np.log(Uni)} \\n\")\n",
    "\n",
    "print(f\"The log likelihood of the sentence 'THE STOCK MARKET FELL BY ONE HUNDRED POINTS LAST WEEK' by bigram model is {np.log(Bi)} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b93461-caa4-4db5-97f5-1fb746158c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE' by unigram model is -44.291934473132606 \n",
      "\n",
      "The pair 'SIXTEEN, OFFICIALS' has zero bigram probability\n",
      "The pair 'SOLD, FIRE' has zero bigram probability\n",
      "The log likelihood of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE' by bigram model is -\\infty \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# part d solution: Comparing log-likelihoods of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE'\n",
    "# under unigram and bigram models\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# loading the vocabulary list as a dictionary with the line numbers in the text file 'Vocab' as keys\n",
    "v=1\n",
    "vocablist=[]\n",
    "with open('hw4_vocab.txt') as f:\n",
    "    for line in f:\n",
    "        vocablist.append(v)\n",
    "        vocablist.append(line.strip())\n",
    "        v=v+1\n",
    "\n",
    "def Convertlisttodic(lst):     # function to convert a list to dictionary\n",
    "    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return res_dct\n",
    " \n",
    "vocabdictwlinekey=Convertlisttodic(vocablist) # creates a dictionary from vocab file with keys as line numbers and words as values \n",
    "\n",
    "##unigram\n",
    "# loading the unigram counts as a dictionary with the line numbers in the text file as keys\n",
    "\n",
    "v=1\n",
    "vocabunigramlist=[]\n",
    "with open('hw4_unigram.txt') as f:\n",
    "    for line in f:\n",
    "        vocabunigramlist.append(v)\n",
    "        vocabunigramlist.append(int(line.strip()))  #takes the counts as integer values \n",
    "        v=v+1\n",
    "\n",
    "unigmdictlinekey=Convertlisttodic(vocabunigramlist) # creates a dictionary from the text file with keys as line numbers and unigram counts as values \n",
    "\n",
    "\n",
    "# computing unigram probabilities: \n",
    "s1=0    # total count of all the words\n",
    "for key in unigmdictlinekey.keys():\n",
    "    s1=s1+unigmdictlinekey[key]\n",
    "probdictuni={key: value / s1 for key, value in unigmdictlinekey.items()}   \n",
    "# the last line above creates probabilities/ML estimates from unigram count\n",
    "\n",
    "\n",
    "def unigramprobcalc(word):\n",
    "    s=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value == word:\n",
    "            s=1\n",
    "            break\n",
    "    if s==0:\n",
    "        print('the word is not in the unigram count list/vocabulary so unigram probabilty will be 0')\n",
    "        return 0\n",
    "    return probdictuni[key]\n",
    "  \n",
    "#################################################################\n",
    "## bigram\n",
    "# loading the bigram counts as an array : 1st column values in the array are indices for words that appear first \n",
    "# and 2nd column values in the array are indices for words that appear next. 3rd column values are numbers of such occurrences\n",
    "\n",
    "X = pd.read_csv('hw4_bigram.txt', sep=\"\\t\", header=None) # reading the table of the file using Pandas\n",
    "\n",
    "# so our array of bigramcount is :\n",
    "bigrampairscount=np.array(X.values) #: Note in the array 'bigrampairscount', indexing of rows and columns start from 0\n",
    "\n",
    "def bitotwordcount(word):     # calculates the total count of a preceeding word (w) in the vocabulary \n",
    "    countword=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==word:\n",
    "            k=key   # this key is unique for each word\n",
    "            break\n",
    "    for l in range(len(bigrampairscount[:,0])):\n",
    "        if  bigrampairscount[l,0]== k:\n",
    "            countword=countword+bigrampairscount[l,2]\n",
    "    return countword\n",
    "\n",
    "# computing total bigram ocuurences for each word that preceedes:  and storing it in a dictionary\n",
    "bitotfirstword={key: bitotwordcount(value) for key, value in vocabdictwlinekey.items()}   \n",
    "\n",
    "    \n",
    "# making a final array of maximal likelihood, bigram probability distribution\n",
    "# it will store bigram probability P_b(w'|w) where index of w can be found from the first\n",
    "# row of 'bigrampairscount' and index of w' can be found from second row of that array\n",
    "# after getting index we get the actual words from the dictionary 'vocabdictwlinekey'.\n",
    "\n",
    "probdist=np.zeros(len(bigrampairscount[:,0]), dtype=float)\n",
    "\n",
    "for i in range(len(bigrampairscount[:,0])):\n",
    "    dummy=bitotfirstword[bigrampairscount[i,0]]\n",
    "    probdist[i]=bigrampairscount[i,2]/dummy\n",
    "\n",
    "# this probdist will give us all the probabilities of pairs (w,w') i.e. maximal likelihood P(w'|w)\n",
    "\n",
    "# if we want to make the probabilities of the pair of words as in the bigram text file then:\n",
    "\n",
    "bigramprob=np.column_stack((bigrampairscount[:,0:2],probdist))\n",
    "# -in this last array : 1st column = index of w, second column=index of w'\n",
    "# 3rd column is P(w'|w)\n",
    "\n",
    "def findkey(i,j):      # finds the probability of a part w, w' with indices i, j respectively: \n",
    "    t=0\n",
    "    for m in range(len(bigramprob[:,0])):\n",
    "        if bigramprob[m,0]==i and bigramprob[m,1]==j:\n",
    "            t=1\n",
    "            break\n",
    "    if t==1:\n",
    "        return m\n",
    "    else:\n",
    "        return len(bigramprob[:,0])+5\n",
    "    \n",
    "\n",
    "def bigramprobcalc(wordfirst, wordsecond): # calculates P(wordsecond|wordfirst) from bigram data\n",
    "    s=0\n",
    "    keyfirst=0\n",
    "    keysecond=0    #our keys for words start from 1 so 0 is a safe value\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==wordfirst:\n",
    "            keyfirst=key\n",
    "            break\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==wordsecond:\n",
    "            keysecond=key\n",
    "            break\n",
    "    if keyfirst!=0 and keysecond!=0:\n",
    "        #assuming uniqueness of a pair in the bigram text file\n",
    "        f=findkey(keyfirst,keysecond)\n",
    "        if f !=len(bigramprob[:,0])+5:\n",
    "            s=bigramprob[f,2]\n",
    "        \n",
    "    return s\n",
    "\n",
    "####################################### Actual calculation of part d of the problem##############\n",
    "\n",
    "# the sentence : \"THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE. \"\n",
    "\n",
    "p1=np.array([unigramprobcalc('THE'),unigramprobcalc('SIXTEEN'),unigramprobcalc('OFFICIALS'),unigramprobcalc('SOLD'),unigramprobcalc('FIRE'), unigramprobcalc('INSURANCE')])\n",
    "\n",
    "s=1\n",
    "for i in range(6):\n",
    "    if i == 0 and p1[i]==0:\n",
    "        s=0\n",
    "        print(\"The word 'THE' has zero unigram probability\")\n",
    "    elif i == 1 and p1[i]==0:\n",
    "        s=0\n",
    "        print(\"The word 'SIXTEEN' has zero unigram probability\")\n",
    "    elif i == 2 and p1[i]==0:\n",
    "        s=0\n",
    "        print(\"The word 'OFFICIALS' has zero unigram probability\")\n",
    "    elif i == 3 and p1[i]==0:\n",
    "        s=0\n",
    "        print(\"The word 'SOLD' has zero unigram probability\")\n",
    "    elif i == 4 and p1[i]==0:\n",
    "        s=0\n",
    "        print(\"The word 'FIRE' has zero unigram probability\")\n",
    "    elif i == 5 and p1[i]==0:\n",
    "        s=0\n",
    "        print(\"The word 'INSURANCE' has zero unigram probability\")\n",
    "\n",
    "if s==1:\n",
    "    Uni=np.prod(p1)\n",
    "    print(f\"The log likelihood of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE' by unigram model is {np.log(Uni)} \\n\")\n",
    "else:\n",
    "    print(\"The log likelihood of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE' by unigram model is -\\infty \\n\")\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "\n",
    "p2=np.array([bigramprobcalc('<s>', 'THE'),bigramprobcalc('THE', 'SIXTEEN'),bigramprobcalc('SIXTEEN', 'OFFICIALS'),bigramprobcalc('OFFICIALS', 'SOLD'),bigramprobcalc('SOLD', 'FIRE'),bigramprobcalc('FIRE', 'INSURANCE')])\n",
    "\n",
    "t=1\n",
    "for i in range(6):\n",
    "    if i == 0 and abs(p2[i])<pow(10,-12):\n",
    "        t=0\n",
    "        print(\"The pair '<s>, THE' has zero bigram probability\")\n",
    "    elif i == 1 and abs(p2[i])<pow(10,-12):\n",
    "        t=0\n",
    "        print(\"The pair 'THE, SIXTEEN' has zero bigram probability\")\n",
    "    elif i == 2 and abs(p2[i])<pow(10,-12):\n",
    "        t=0\n",
    "        print(\"The pair 'SIXTEEN, OFFICIALS' has zero bigram probability\")\n",
    "    elif i == 3 and abs(p2[i])<pow(10,-12):\n",
    "        t=0\n",
    "        print(\"The pair 'OFFICIALS, SOLD' has zero bigram probability\")\n",
    "    elif i == 4 and abs(p2[i])<pow(10,-12):\n",
    "        t=0\n",
    "        print(\"The pair 'SOLD, FIRE' has zero bigram probability\")\n",
    "    elif i == 5 and abs(p2[i])<pow(10,-12):\n",
    "        t=0\n",
    "        print(\"The pair 'FIRE, INSURANCE' has zero bigram probability\")\n",
    "\n",
    "if t==1:\n",
    "    Bi=np.prod(p2)\n",
    "    print(f\"The log likelihood of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE' by bigram model is {np.log(Bi)} \\n\")\n",
    "else:\n",
    "    print(\"The log likelihood of the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE' by bigram model is -\\infty \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "269fbe1c-cfd4-4f2a-95fd-008f2f7ab9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "based on the calculation above, maximum value of L_m is: -42.96413720705673\n",
      "the value of \\Lambda where this maximum occurs is: 0.648\n"
     ]
    }
   ],
   "source": [
    "# part e solution: plotting the required data for the sentence 'THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE'\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "\n",
    "# loading the vocabulary list as a dictionary with the line numbers in the text file 'Vocab' as keys\n",
    "v=1\n",
    "vocablist=[]\n",
    "with open('hw4_vocab.txt') as f:\n",
    "    for line in f:\n",
    "        vocablist.append(v)\n",
    "        vocablist.append(line.strip())\n",
    "        v=v+1\n",
    "\n",
    "def Convertlisttodic(lst):     # function to convert a list to dictionary\n",
    "    res_dct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    return res_dct\n",
    " \n",
    "vocabdictwlinekey=Convertlisttodic(vocablist) # creates a dictionary from vocab file with keys as line numbers and words as values \n",
    "\n",
    "##unigram\n",
    "# loading the unigram counts as a dictionary with the line numbers in the text file as keys\n",
    "\n",
    "v=1\n",
    "vocabunigramlist=[]\n",
    "with open('hw4_unigram.txt') as f:\n",
    "    for line in f:\n",
    "        vocabunigramlist.append(v)\n",
    "        vocabunigramlist.append(int(line.strip()))  #takes the counts as integer values \n",
    "        v=v+1\n",
    "\n",
    "unigmdictlinekey=Convertlisttodic(vocabunigramlist) # creates a dictionary from the text file with keys as line numbers and unigram counts as values \n",
    "\n",
    "\n",
    "# computing unigram probabilities: \n",
    "s1=0    # total count of all the words\n",
    "for key in unigmdictlinekey.keys():\n",
    "    s1=s1+unigmdictlinekey[key]\n",
    "\n",
    "probdictuni={key: (value)/ s1 for key, value in unigmdictlinekey.items()}   \n",
    "# the last line above creates probabilities/ML estimates from unigram count\n",
    "\n",
    "\n",
    "def unigramprobcalc(word):\n",
    "    s=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value == word:\n",
    "            s=1\n",
    "            break\n",
    "    if s==0:\n",
    "        print('the word is not in the unigram count list/vocabulary so unigram probabilty will be 0')\n",
    "        return 0\n",
    "    return probdictuni[key]\n",
    "  \n",
    "#################################################################\n",
    "## bigram\n",
    "# loading the bigram counts as an array : 1st column values in the array are indices for words that appear first \n",
    "# and 2nd column values in the array are indices for words that appear next. 3rd column values are numbers of such occurrences\n",
    "\n",
    "X = pd.read_csv('hw4_bigram.txt', sep=\"\\t\", header=None) # reading the table of the file using Pandas\n",
    "\n",
    "# so our array of bigramcount is :\n",
    "bigrampairscount=np.array(X.values) #: Note in the array 'bigrampairscount', indexing of rows and columns start from 0\n",
    "\n",
    "def bitotwordcount(word):     # calculates the total count of a preceeding word (w) in the vocabulary \n",
    "    countword=0\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==word:\n",
    "            k=key   # this key is unique for each word\n",
    "            break\n",
    "    for l in range(len(bigrampairscount[:,0])):\n",
    "        if  bigrampairscount[l,0]== k:\n",
    "            countword=countword+bigrampairscount[l,2]\n",
    "    return countword\n",
    "\n",
    "# computing total bigram ocuurences for each word that preceedes:  and storing it in a dictionary\n",
    "bitotfirstword={key: bitotwordcount(value) for key, value in vocabdictwlinekey.items()}   \n",
    "\n",
    "    \n",
    "# making a final array of maximal likelihood, bigram probability distribution\n",
    "# it will store bigram probability P_b(w'|w) where index of w can be found from the first\n",
    "# row of 'bigrampairscount' and index of w' can be found from second row of that array\n",
    "# after getting index we get the actual words from the dictionary 'vocabdictwlinekey'.\n",
    "\n",
    "probdist=np.zeros(len(bigrampairscount[:,0]), dtype=float)\n",
    "\n",
    "for i in range(len(bigrampairscount[:,0])):\n",
    "    dummy=bitotfirstword[bigrampairscount[i,0]]\n",
    "    probdist[i]=bigrampairscount[i,2]/dummy\n",
    "\n",
    "# this probdist will give us all the probabilities of pairs (w,w') i.e. maximal likelihood P(w'|w)\n",
    "\n",
    "# if we want to make the probabilities of the pair of words as in the bigram text file then:\n",
    "\n",
    "bigramprob=np.column_stack((bigrampairscount[:,0:2],probdist))\n",
    "# -in this last array : 1st column = index of w, second column=index of w'\n",
    "# 3rd column is P(w'|w)\n",
    "\n",
    "def findkey(i,j):      # finds the probability of a part w, w' with indices i, j respectively: \n",
    "    t=0\n",
    "    for m in range(len(bigramprob[:,0])):\n",
    "        if bigramprob[m,0]==i and bigramprob[m,1]==j:\n",
    "            t=1\n",
    "            break\n",
    "    if t==1:\n",
    "        return m\n",
    "    else:\n",
    "        return len(bigramprob[:,0])+5\n",
    "    \n",
    "\n",
    "def bigramprobcalc(wordfirst, wordsecond): # calculates P(wordsecond|wordfirst) from bigram data\n",
    "    s=0\n",
    "    keyfirst=0\n",
    "    keysecond=0    #our keys for words start from 1 so 0 is a safe value\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==wordfirst:\n",
    "            keyfirst=key\n",
    "            break\n",
    "    for key, value in vocabdictwlinekey.items():\n",
    "        if value==wordsecond:\n",
    "            keysecond=key\n",
    "            break\n",
    "    if keyfirst!=0 and keysecond!=0:\n",
    "        #assuming uniqueness of a pair in the bigram text file\n",
    "        f=findkey(keyfirst,keysecond)\n",
    "        if f !=len(bigramprob[:,0])+5:\n",
    "            s=bigramprob[f,2]\n",
    "        \n",
    "    return s\n",
    "\n",
    "####################################### Actual calculation of part e of the problem##############\n",
    "\n",
    "# the sentence : \"THE SIXTEEN OFFICIALS SOLD FIRE INSURANCE. \"\n",
    "\n",
    "p1=np.array([unigramprobcalc('THE'),unigramprobcalc('SIXTEEN'),unigramprobcalc('OFFICIALS'),unigramprobcalc('SOLD'),unigramprobcalc('FIRE'), unigramprobcalc('INSURANCE')])\n",
    "\n",
    "p2=np.array([bigramprobcalc('<s>', 'THE'),bigramprobcalc('THE', 'SIXTEEN'),bigramprobcalc('SIXTEEN', 'OFFICIALS'),bigramprobcalc('OFFICIALS', 'SOLD'),bigramprobcalc('SOLD', 'FIRE'),bigramprobcalc('FIRE', 'INSURANCE')])\n",
    "\n",
    "el=np.linspace(0.001, 1, num=1000)  # this is the vector of lambda\n",
    "\n",
    "logprod=np.zeros(len(el))\n",
    "\n",
    "for i in range(len(el)):\n",
    "    pm=np.add(el[i]*p1,(1-el[i])*p2)\n",
    "    prodpm=np.prod(pm)\n",
    "    logprod[i]=np.log(prodpm)\n",
    "\n",
    "plt.title(f\"Log likelihood L_m as function of parameter Lambda \\in [0,1]\") \n",
    "plt.xlabel(f\"Lambda\") \n",
    "plt.ylabel(f\"L_m\") \n",
    "plt.plot(el,logprod)\n",
    "plt.show()\n",
    "\n",
    "print(f\"based on the calculation above, maximum value of L_m is: {np.max(logprod)}\")\n",
    "print(f\"the value of \\Lambda where this maximum occurs is: {el[np.argmax(logprod)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d685bb-f2ea-404f-9243-095b31183b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
